{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get all the functions into the module. Apparently, we need to put the stuff together. The first function is the buildModel function, which will build the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(X_train,y_train,X_test,y_test,batch_size):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    #model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "    model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical-crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n",
    "              validation_data=(X_test, y_test))\n",
    "    score, acc = model.evaluate(X_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need some basic function including load_review_dataset, create_dictionary, transform_text to get the dataset ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review_dataset(path):\n",
    "    with open(path,'r',encoding='utf8') as f:\n",
    "        content = f.readlines()\n",
    "    res = []\n",
    "    for line in content:\n",
    "        a = line.strip('.').lower().split()\n",
    "        b = []\n",
    "        exclude = set(string.punctuation)\n",
    "        for word in a:\n",
    "            word = ''.join(ch for ch in word if ch not in exclude)\n",
    "            b.append(word)\n",
    "        res.append(b)\n",
    "    # res = [line.strip('.').lower().split() for line in content]\n",
    "    return res\n",
    "\n",
    "def create_dictionary(messages):\n",
    "    tempDict = collections.defaultdict(int)\n",
    "    for message in messages:\n",
    "        setWords = set(message)\n",
    "        for word in setWords:\n",
    "            tempDict[word] += 1\n",
    "    resDict = collections.defaultdict(int)\n",
    "    i = 0\n",
    "    for key,val in tempDict.items():\n",
    "        if(val>=6):\n",
    "            resDict[key] = i\n",
    "            i +=1\n",
    "    return resDict\n",
    "\n",
    "def transform_text(messages, word_dictionary):\n",
    "    numRows = len(messages)\n",
    "    print(numRows)\n",
    "    numCols = max(word_dictionary.values())+1\n",
    "    print(numCols)\n",
    "    resArray = np.zeros((numRows,numCols))\n",
    "    for i in range(len(messages)):\n",
    "        message = messages[i]\n",
    "        for word in message:\n",
    "            if(word in word_dictionary):\n",
    "                col = word_dictionary[word]\n",
    "                resArray[i][col] +=1\n",
    "    return resArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load the dataset including the train_review, train_rate,test_review, test_rate using the function loadTrainTestDataSet()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainTestDataSet():\n",
    "    train_review_path = './data/train.review.txt'\n",
    "    train_rate_path = './data/train.rating.txt'\n",
    "    dev_review_path = './data/dev.review.txt'\n",
    "    dev_rate_path = './data/dev.rating.txt'\n",
    "    test_review_path = './data/test.review.txt'\n",
    "    test_rate_path = './data/test.rating.txt'\n",
    "    review_path = './data/review_full.txt'\n",
    "    rate_path = './data/rate_full.txt'\n",
    "    messages = load_review_dataset(review_path)\n",
    "    word_dict = create_dictionary(messages)\n",
    "\n",
    "    train_messages = load_review_dataset(train_review_path)\n",
    "    train_resArray = transform_text(train_messages, word_dict)\n",
    "    dev_messages = load_review_dataset(dev_review_path)\n",
    "    dev_resArray = transform_text(dev_messages, word_dict)\n",
    "    test_messages = load_review_dataset(test_review_path)\n",
    "    test_resArray = transform_text(test_messages, word_dict)\n",
    "\n",
    "    # resArray = transform_text(messages, word_dict)\n",
    "    # print('res array shape ',resArray.shape)\n",
    "    # glove = getAverageGlove(messages)\n",
    "    # print('glove', glove.shape)\n",
    "    # To construct the training dataset\n",
    "    # numOfDataPoints = len(resArray)\n",
    "    # numOfTraining = int(numOfDataPoints * 0.8)\n",
    "    trainingDataX = train_resArray[:, ]\n",
    "    testDataX = test_resArray[:, ]\n",
    "    trainRateY = pd.read_csv(train_rate_path, header=None)\n",
    "    testRateY = pd.read_csv(test_rate_path, header=None)\n",
    "    # print(len(traRateY))\n",
    "    trainingRateY = trainRateY[:][0]\n",
    "    testRateY = testRateY[:][0]\n",
    "    return (trainingDataX,trainingRateY,testDataX,testRateY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the loading dataset and building model together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X_train,y_train,X_test,y_test = loadTrainTestDataSet()\n",
    "    batch_size = 32\n",
    "    buildModel(X_train, y_train, X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n",
      "4223\n",
      "6814\n",
      "4223\n",
      "6813\n",
      "4223\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-46930829cb1c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadTrainTestDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-489a434f4a73>\u001b[0m in \u001b[0;36mbuildModel\u001b[0;34m(X_train, y_train, X_test, y_test, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#model.add(Embedding(max_features, 128, dropout=0.2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# try using a GRU instead, for fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    452\u001b[0m                     \u001b[0;31m# know about its input shape. Otherwise, that's an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_input_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                         raise ValueError('The first layer in a '\n\u001b[0m\u001b[1;32m    455\u001b[0m                                          \u001b[0;34m'Sequential model must '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                                          \u001b[0;34m'get an `input_shape` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
